# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CRu1p18jeiw_59pJMZcM5DAFERS0MVYl
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import tensorflow_hub as hub

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten, GlobalAveragePooling1D

import numpy as np
import pandas as pd
import matplotlib.pylab as plt
import os, re, json, functools

plt.rc_context({'xtick.color':'w', 'ytick.color':'w', 'text.color':'w', 'axes.labelcolor':'w'})

seed=2811
np.random.seed(seed)
tf.random.set_seed(seed)

pip install kaggle --upgrade

os.environ['KAGGLE_USERNAME'] = "karenvaldez"
os.environ['KAGGLE_KEY'] = "5912caabf0d1fa350842f382da374953"

#https://www.kaggle.com/rounakbanik/the-movies-dataset
!kaggle datasets download rounakbanik/the-movies-dataset

!unzip -o 'the-movies-dataset.zip'

os.listdir()

movies = pd.read_csv('movies_metadata.csv')
movies.head

movies.columns

movies.genres

movies.overview

movies = movies[movies['imdb_id'] != '0']
movies = movies[movies['genres'] != "[]"]
movies = movies[~pd.isna(movies['overview'])]


dataset = movies[['overview', 'genres']]

len(dataset)

CLASS_NAMES = ['Drama','Comedy','Thriller','Romance','Action']
CLASS_COUNT = len(CLASS_NAMES)
CLASS_COUNT

dataset.iloc[0]['genres']

dataset['genres'].replace("\'", "\"", regex=True, inplace=True)
dataset.iloc[0]['genres']

#genre_to_id = dict()
label_count = dict()

def genre_to_list(str_genre):
  keep = True
  genre_list = list(map(lambda genre:genre['name'],json.loads(str_genre)))
  
  for genre in genre_list:
    count = label_count.get(genre, 0) + 1
    label_count[genre] = count
    if (count > 5000):
      keep = False
    #if (genre_to_id.get(genre,-1) == -1):
    #  genre_to_id[genre] = len(genre_to_id.keys())
  genres_as_ids = list(map(lambda x: CLASS_NAMES.index(x), filter(lambda x: x in CLASS_NAMES, genre_list)))
  #genres_as_ids = list(map(lambda x:genre_to_id[x], genre_list)) 
  genres_one_hot = list()

  if (len(genres_as_ids) > 0 and keep):
    genres_one_hot = tf.keras.utils.to_categorical(genres_as_ids, CLASS_COUNT, dtype='int')
    genres_one_hot = functools.reduce(lambda x,y : x+y,genres_one_hot)

  return genres_one_hot

dataset['genres'] = dataset['genres'].apply(lambda x: genre_to_list(x))
dataset.iloc[0]['genres']

dataset = dataset[~dataset['genres'].str.len().eq(0)]
len(dataset)

sorted(label_count.items(), key=lambda x: x[1], reverse=True)

#CLASS_NAMES = list(genre_to_id.keys())
#CLASS_COUNT == len(CLASS_NAMES), CLASS_NAMES

train_split = dataset.sample(frac = 0.7,random_state=42)
test_split = dataset.drop(train_split.index).sample(frac = 0.5)
validation_split = dataset.drop(train_split.index).drop(test_split.index)

train_split.shape, validation_split.shape, test_split.shape

train_split.iloc[0]

train_labels = train_split['genres']
test_labels = test_split['genres']
validation_labels = validation_split['genres']
train_labels.shape, test_labels.shape, validation_labels.shape

train_dataset = tf.data.Dataset.from_tensor_slices((train_split['overview'], train_split['genres']))
test_dataset = tf.data.Dataset.from_tensor_slices((test_split['overview'], test_split['genres']))
validation_dataset = tf.data.Dataset.from_tensor_slices((validation_split['overview'], validation_split['genres']))
train_dataset.element_spec

overview, genres = next(iter(train_dataset))
overview, genres

batch_size = 128
shuffle_buffer_size = 1000

train_dataset = train_dataset.shuffle(shuffle_buffer_size).repeat().batch(batch_size)
validation_dataset = validation_dataset.shuffle(shuffle_buffer_size).batch(batch_size)
test_dataset = test_dataset.shuffle(shuffle_buffer_size).batch(batch_size)

overview, genres = next(iter(train_dataset))
overview.shape, genres.shape

epochs = 20
train_steps = 10
validation_steps = 5

# https://tfhub.dev/tensorflow/tfjs-model/toxicity/1/default/1
# embedding = "https://tfhub.dev/google/elmo/3"
embedding = "https://tfhub.dev/google/Wiki-words-500-with-normalization/2"
hub_layer = hub.KerasLayer(embedding, input_shape=[], 
                           dtype=tf.string, trainable=False)

model = tf.keras.Sequential([
                             hub_layer,
                             Flatten(),
                             Dense(64, activation='relu'),  
                             Dense(CLASS_COUNT, activation='sigmoid'),
])

model.summary()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_dataset,
                    epochs=epochs,
                    steps_per_epoch=3,
                    validation_data=validation_dataset,
                    validation_steps=validation_steps
                   )

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(10, 10))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.setp(plt.legend().get_texts(), color='black')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1.0])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.setp(plt.legend().get_texts(), color='black')
plt.ylabel('Cross Entropy')
plt.ylim([0.0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

test_loss, test_acc = model.evaluate(test_dataset, verbose=3)
print('Loss:', test_loss)
print('Accuracy:', test_acc)